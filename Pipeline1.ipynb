{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Pipeline 1:__\n",
    "## _Normalization --> Z Score --> 3 STD_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required package for data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "%matplotlib inline\n",
    "\n",
    "# import required packages for splitting data\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import required packages for evaluating models\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# import `logistic regression` model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata_original = pd.read_csv('comp1data.csv', header=0, na_values = '-')\n",
    "compdata_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata_original.shape\n",
    "# 682 rows and 22 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata_original.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata_original.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data\n",
    "compdata = compdata_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the rows T2 values!!!! mean\n",
    "# Cannot do calculations with missing data on the number of words, the ratios will be 0 and create outliers\n",
    "\n",
    "compdata.loc[compdata['T2'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.loc[125,'T2']=compdata['T2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.loc[199,'T2']=compdata['T2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for negatives\n",
    "# cannot have a negative number of long words (T5)\n",
    "\n",
    "compdata.loc[compdata['T5'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.loc[7,'T5']=compdata['T5'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot have a negative number of positive words (S1)\n",
    "compdata.loc[compdata['S1'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the 0, negative, and infinite values with the mean for continuous values\n",
    "compdata.loc[134,'S1']=compdata['S1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata['S1'] = pd.to_numeric(compdata['S1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratios:\n",
    "\n",
    "    # ** How to find the ratio for T1 and T2... impute them\n",
    "\n",
    "# Percentage of real words\n",
    "compdata['T3_'] = (compdata['T3']/compdata['T2'])\n",
    "# Percentage of long sentences\n",
    "compdata['T4_'] = (compdata['T4']/compdata['T1'])\n",
    "# Percentage of long words\n",
    "compdata['T5_'] = (compdata['T5']/compdata['T2'])\n",
    "# Percentage of positive words\n",
    "compdata['S1_'] = (compdata['S1']/compdata['T2'])\n",
    "# Percentage of negative words\n",
    "compdata['S2_'] = (compdata['S2']/compdata['T2'])\n",
    "# Percentage of uncertain words\n",
    "compdata['S3_'] = (compdata['S3']/compdata['T2'])\n",
    "compdata.head()\n",
    "\n",
    "# Target & Control Variable Calculation\n",
    "\n",
    "compdata['P(mid)'] = ((compdata['P(H)']+compdata['P(L)'])/2)\n",
    "\n",
    "def p(row):\n",
    "    if row['P(IPO)'] < row['P(mid)']:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "def q(row):\n",
    "    if row['P(1Day)'] > row['P(IPO)']:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "def r(row):\n",
    "    if row['C3'] >= 0:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "def s(row):\n",
    "    if row['P(IPO)'] > row['P(mid)']:\n",
    "        val = (row['P(IPO)'] - row['P(mid)'])/row['P(mid)']*100\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "compdata['C3_'] = compdata.apply(r, axis=1)\n",
    "compdata['C5_'] = compdata['C5']/compdata['C6'] \n",
    "compdata['C6_'] = compdata.apply(s, axis=1)\n",
    "compdata['Y1'] = compdata.apply(p, axis=1)\n",
    "compdata['Y2'] = compdata.apply(q, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the percentage of missing data in each column\n",
    "compdata.isna().mean().round(4) * 100\n",
    "\n",
    "# Majority of the data is missing from C7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isinstance(compdata['I3'], object) \n",
    "\n",
    "#compdata['I3'] = [isinstance(x, str) for x in compdata.I3]\n",
    "#compdata\n",
    "\n",
    "#compdata[compdata.I3.apply(lambda x: isinstance(x, str))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata[compdata[\"I1\"]=='DLB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata[compdata[\"I1\"]=='DTSI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata[compdata[\"I1\"]=='RLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata[compdata[\"I1\"]=='UPG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.I3.replace(to_replace =\"6794, 3861, 3663, 7819\", value =\"3000\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.I3.replace(to_replace =\"3651, 6794, 7819\", value =\"9900\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.I3.replace(to_replace =\"3663, 3861, 6794\", value =\"3000\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.I3.replace(to_replace =\"7389, 5063\", value =\"9900\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.I3.replace(to_replace =\"541990\", value =\"7000\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata[\"I3\"] = pd.to_numeric(compdata[\"I3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_I3_bins(I3):\n",
    "    if I3 >= 100 and I3 <= 999:                    \n",
    "        return(1)\n",
    "    if I3 >= 1000 and I3 <= 1499:       \n",
    "        return(2)\n",
    "    if I3 >= 1500 and I3 <= 1799:       \n",
    "        return(3)\n",
    "    if I3 >= 1800 and I3 <= 1999:      \n",
    "        return(4)\n",
    "    if I3 >= 2000 and I3 <= 3999:                  \n",
    "        return(5)\n",
    "    if I3 >= 4000 and I3 <= 4999:                  \n",
    "        return(6)\n",
    "    if I3 >= 5000 and I3 <= 5199:                 \n",
    "        return(7)\n",
    "    if I3 >= 5200 and I3 <= 5999:                  \n",
    "        return(8)\n",
    "    if I3 >= 6000 and I3 <= 6799:                 \n",
    "        return(9)\n",
    "    if I3 >= 7000 and I3 <= 8999:                  \n",
    "        return(10)\n",
    "    if I3 >= 9100 and I3 <= 9729:                  \n",
    "        return(11)   \n",
    "    if I3 >= 9900 and I3 <= 9999:                  \n",
    "        return(12)   \n",
    "    \n",
    "    # Creating a new column I3_bins in the dataframe\n",
    "\n",
    "compdata['I3_bins'] = compdata['I3'].apply(assign_I3_bins)  \n",
    "\n",
    "compdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since C2 is a binary value, we will fill the missing data with the mode\n",
    "\n",
    "compdata['Imputed_C2'] = compdata['C2'].fillna(compdata['C2'].mode()[0])\n",
    "compdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata['Imputed_C7'] = compdata['C7'].fillna(compdata['C7'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing P Values from Continues DF\n",
    "compdata = compdata.dropna(subset=['P(1Day)'], axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata.reset_index(drop=True, inplace=True)\n",
    "compdata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF with only continuous features\n",
    "\n",
    "# Drop the imputed variable colmuns\n",
    "# Drop the P values\n",
    "# Drop Y1 Y2 **make seperate df\n",
    "    # Drop T1 and T2?\n",
    "\n",
    "compdata_cont = compdata.drop(['P(IPO)','P(H)','P(L)','P(1Day)','P(mid)','I1','I2','I3','T3','T1','T2','T4','T5','C3_','S1','S2','S3','C2','C3','C5','C6','C7','Imputed_C2','I3_bins','Y1','Y2'], axis=1)\n",
    "compdata_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata_cont.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF with only categorical features\n",
    "# drop y1 y2 c2 i3\n",
    "compdata_cat = compdata.drop(['P(IPO)','P(H)','P(L)','P(1Day)','P(mid)','I3','C2','C1','C3','C4','C5','C6','C7','T1','T2','T3','T4','T5','S1','S2','S3','T3_','T4_','T5_','S1_','S2_','S3_','C5_','C6_','Y1','Y2','Imputed_C7'], axis=1)\n",
    "compdata_cat.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetvals = compdata.drop(['P(IPO)','P(H)','P(L)','P(1Day)','P(mid)','I3','C2','C1','C3','C4','C5','C6','C7','T1','T2','T3','T4','T5','S1','S2','S3','T3_','T4_','T5_','S1_','S2_','S3_','C5_','C6_','I1','I2','C3_','I3_bins','Imputed_C2'], axis=1)\n",
    "targetvals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run evaluation code now.. go back and check the results to see if it improved or not\n",
    "\n",
    "##### after impute, after min max, after normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1: Normalization -> Z Score -> 3 STD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata_cont.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library and data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# plot\n",
    "f, axes = plt.subplots(3, 4, figsize=(15,10), sharex=False)\n",
    "sns.distplot( compdata_cont[\"C1\"] , ax=axes[0, 0])\n",
    "sns.distplot( compdata_cont[\"C4\"] , ax=axes[0, 1])\n",
    "sns.distplot( compdata_cont[\"T3_\"] , ax=axes[0, 2])\n",
    "sns.distplot( compdata_cont[\"T4_\"] , ax=axes[0, 3])\n",
    "sns.distplot( compdata_cont[\"T5_\"] , ax=axes[1, 0])\n",
    "sns.distplot( compdata_cont[\"S1_\"] , ax=axes[1, 1])\n",
    "sns.distplot( compdata_cont[\"S2_\"] , ax=axes[1, 2])\n",
    "sns.distplot( compdata_cont[\"S3_\"] , ax=axes[1, 3])\n",
    "sns.distplot( compdata_cont[\"C5_\"] , ax=axes[2, 0])\n",
    "sns.distplot( compdata_cont[\"C6_\"] , ax=axes[2, 1])\n",
    "sns.distplot( compdata_cont[\"Imputed_C7\"] , ax=axes[2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compdata_cont.skew()\n",
    "\n",
    "# check if its left or right skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive skew\n",
    "\n",
    "compdata_cont['C1'] = compdata_cont['C1'].apply(np.cbrt)\n",
    "compdata_cont['C1'] = compdata_cont['C1'].apply(np.cbrt)\n",
    "compdata_cont['C1'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative skew\n",
    "\n",
    "compdata_cont['T3_'] = compdata_cont['T3_'].apply(np.exp)\n",
    "compdata_cont['T3_'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative skew\n",
    "\n",
    "compdata_cont['T4_'] = compdata_cont['T4_'].apply(np.exp)\n",
    "compdata_cont['T4_'] = compdata_cont['T4_'].apply(np.exp)\n",
    "compdata_cont['T4_'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive skew\n",
    "\n",
    "compdata_cont['S1_'] = compdata_cont['S1_'].apply(np.log)\n",
    "compdata_cont['S1_'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive skew\n",
    "\n",
    "compdata_cont['S2_'] = compdata_cont['S2_'].apply(np.log)\n",
    "compdata_cont['S2_'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive skew\n",
    "\n",
    "compdata_cont['S3_'] = compdata_cont['S3_'].apply(np.log)\n",
    "compdata_cont['S3_'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive skew\n",
    "\n",
    "compdata_cont['C5_'] = compdata_cont['C5_'].apply(np.cbrt)\n",
    "compdata_cont['C5_'] = compdata_cont['C5_'].apply(np.cbrt)\n",
    "compdata_cont['C5_'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive skew\n",
    "\n",
    "compdata_cont['C6_'] = compdata_cont['C6_'].apply(np.cbrt)\n",
    "compdata_cont['C6_'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library and data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# plot\n",
    "f, axes = plt.subplots(3, 4, figsize=(15,10), sharex=False)\n",
    "sns.distplot( compdata_cont[\"C1\"] , ax=axes[0, 0])\n",
    "sns.distplot( compdata_cont[\"C4\"] , ax=axes[0, 1])\n",
    "sns.distplot( compdata_cont[\"T3_\"] , ax=axes[0, 2])\n",
    "sns.distplot( compdata_cont[\"T4_\"] , ax=axes[0, 3])\n",
    "sns.distplot( compdata_cont[\"T5_\"] , ax=axes[1, 0])\n",
    "sns.distplot( compdata_cont[\"S1_\"] , ax=axes[1, 1])\n",
    "sns.distplot( compdata_cont[\"S2_\"] , ax=axes[1, 2])\n",
    "sns.distplot( compdata_cont[\"S3_\"] , ax=axes[1, 3])\n",
    "sns.distplot( compdata_cont[\"C5_\"] , ax=axes[2, 0])\n",
    "sns.distplot( compdata_cont[\"C6_\"] , ax=axes[2, 1])\n",
    "sns.distplot( compdata_cont[\"Imputed_C7\"] , ax=axes[2, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for infinite numbers in df\n",
    "\n",
    "#close to 0 is normal (T4_).. [-.5 - .5]\n",
    "\n",
    "\n",
    "# log all the values for the continues values *positives\n",
    "# exponential for the negative continues values\n",
    "\n",
    "# make it fit better into range after log and exponential (sqrt? log again?)\n",
    "\n",
    "# get in range ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(compdata_cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.transform(compdata_cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "compdata_cont = compdata_cont.apply(zscore)\n",
    "compdata_cont.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in compdata_cont.columns:\n",
    "    u_bound = compdata_cont[col].mean() + 3* compdata_cont[col].std()\n",
    "    compdata_cont[col][compdata_cont[col] > u_bound] = u_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in compdata_cont.columns:\n",
    "    u_bound = compdata_cont[col].mean() - 3* compdata_cont[col].std()\n",
    "    compdata_cont[col][compdata_cont[col] < l_bound] = l_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Variables\n",
    "\n",
    "index= ['P(IPO)', 'P(H)', 'P(L)', 'P(1Day)','C1','C3','C4','C5','C6','C7','T1','T2','T3','T4','T5','S1','S2','S3']\n",
    "cols = ['P(IPO)', 'P(H)', 'P(L)', 'P(1Day)','C1','C3','C4','C5','C6','C7','T1','T2','T3','T4','T5','S1','S2','S3']\n",
    "df = DataFrame(abs(np.random.randn(18,18)), index=index,  columns=cols)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(df, annot=True, cbar=True, cmap='Blues')\n",
    "plt.title(\"Correlation between Variables\")\n",
    "plt.show()\n",
    "\n",
    "# look for the ones over .5 or under -.5.. these are highly correlated ones\n",
    "# flagging step\n",
    "\n",
    "#every pair highly correlated need to be seperated into a different df\n",
    "\n",
    "# make differe\n",
    "    # 1 pair = 2\n",
    "    # 2 pair = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain why theyre seperated\n",
    "\n",
    "## do feature selection\n",
    "\n",
    "# correlation is alwasy between -1 and 1\n",
    "\n",
    "# merge all 3 parts together categorical back into .. categorical, contin (all the continues made.. not 1), target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zscore\n",
    "\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    ">>> scaler = StandardScaler()\n",
    ">>> print(scaler.fit(data))\n",
    "StandardScaler()\n",
    ">>> print(scaler.mean_)\n",
    "[0.5 0.5]\n",
    ">>> print(scaler.transform(data))\n",
    "[[-1. -1.]\n",
    " [-1. -1.]\n",
    " [ 1.  1.]\n",
    " [ 1.  1.]]\n",
    ">>> print(scaler.transform([[2, 2]]))\n",
    "[[3. 3.]]\n",
    "\n",
    "- report all we did to change the data, not the data describing that we used to impute and change\n",
    "- keep count of missing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
